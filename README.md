| Model | Reference | Core Method | Perturbation Encoding | Perturbation Types | Combination Support | Training Data | Output / Prediction Objective | Benchmark Highlights |
|--------|------------|--------------|------------------------|--------------------|---------------------|----------------|--------------------------------|-----------------------|
| **scGen** | Lotfollahi et al., 2019 | VAE + latent space arithmetic | Mean-difference vectors between perturbed and unperturbed distributions | Cytokine stimulation, infection | No | Human PBMCs, intestinal epithelial cells, phagocytes | Predicts full transcriptome under unseen perturbations | Recovers held-out stimulus responses with high accuracy |
| **CellOT** | Bunne et al., 2023 | Neural Optimal Transport (OT) | Implicit | Drugs, gene knockouts | - | statefate, SciPlex-3 | - | - |
| **CPA** | Lotfollahi et al., 2023 | Conditional VAE with disentangled latent factors | Learned latent embeddings for drugs, doses, genes | Drugs, genetic | Yes | scRNA-seq perturbation compendia (e.g. sci-Plex, CMap) | Predicts post-perturbation expression states | Robust to compound mixtures & unseen cell types |
| **Geneformer** | Theodoris et al., 2023, Chen et al., 2024 | Transformer-based single-cell foundation model pretraining | Contextual token embeddings for genes + perturbations | Drug, genetic | Limited | Multi-tissue scRNA-seq atlases (human + mouse) | Embedding-based latent prediction; expression reconstruction | Stronger generalization vs smaller models on gene-function & perturbation tasks |
| **CellFlow** | Klein et al., 2025 | Neural OT + Flow Matching generative framework | Molecular fingerprints (drugs), ESM2 embeddings (proteins) | Cytokine stimulation, gene KO, drugs | Yes | Human PBMCs, zebrafish embryos, organoids | Conditional flow model for perturbed single-cell phenotypes | SOTA prediction across diverse contexts and developmental stages |
| **STATE** | Adduri et al., 2025 | Transformer foundation model for perturbation prediction | Tokenized representations of cells and perturbations | Genetic, signaling, chemical | Yes | >100 M perturbed cells across experiments | Predicts gene expression changes & DE genes | +30 % accuracy gain vs prior methods; generalizes to unseen contexts |
| **CRISP** | Wang et al., 2025 | VAE based contrastive learning | scFMs + VAE, RDKit | Genetic, drug | Limited | NeurIPS (PBMCs), SciPlex3 (cancer cell lines), GBM, PC9 (NSCLC), PBMC-bench | outputs cell type specific perturbed gene expression profile | - |
| **C2S-Scale** | Rizvi et al., 2025 | LLM trained on Cell2Sentence data (27 B params) | Perturbations and cell states represented as text tokens | Genetic, drug, combinatorial | Yes | >50 M cells + biological text corpora | Generates transcriptomic and textual responses; context-conditioned reasoning | Experimental validation of silmitasertib synergy; SOTA performance |
| **Tx1** | Gandhi et al., 2025 | Masked-expression transformer (scGPT-style) with drug tokens | Joint embeddings for genes, cells, compounds | Small-molecule (cancer) | Yes | Tahoe-100 M (>100 M cells, 1,100 compounds) | Predicts gene essentiality & perturbation responses | 3–30× more compute-efficient than previous cell-state models |
| **LPM** | Miladinovic et al., 2025 | Deep PRC-conditioned architecture | Symbolic representation of perturbation, readout & context | Drugs, genetic | Yes | Multi-omics pooled perturbation experiments | Predicts outcomes of unseen perturbations; infers gene–gene networks | SOTA performance and biological interpretability at scale |

