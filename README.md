| Model | Reference | Core Method | Perturbation Encoding | Perturbation Types | Combination Support | Training Data | Output / Prediction Objective | Benchmark Highlights |
|--------|------------|--------------|------------------------|--------------------|---------------------|----------------|--------------------------------|-----------------------|
| **scGen** | Lotfollahi et al., 2019 | Variational Autoencoder (VAE) + latent space arithmetic | Mean-difference vectors between perturbed and unperturbed distributions | Cytokine stimulation, infection | No | Human PBMCs, intestinal epithelial cells, phagocytes | Predicts full transcriptome under unseen perturbations | Recovers held-out stimulus responses with high accuracy |
| **CellOT** | Bunne et al., 2023 | Neural Optimal Transport (OT) | OT potential functions mapping control → perturbed distributions | Drugs, gene knockouts | Limited | Cross-cell-type perturbation datasets | Models population-level transitions in expression space | Smooth mappings and interpretable trajectory alignment |
| **CPA (Compositional Perturbation Autoencoder)** | Lotfollahi et al., 2023 | Conditional VAE with disentangled latent factors | Learned latent embeddings for drugs, doses, genes | Drugs, genetic | Yes | scRNA-seq perturbation compendia (e.g. sci-Plex, CMap) | Predicts post-perturbation expression states | Robust to compound mixtures & unseen cell types |
| **Geneformer / scGPT** | De Meo et al., 2025 | Transformer-based single-cell foundation model pretraining | Contextual token embeddings for genes + perturbations | Drug, genetic | Limited | Multi-tissue scRNA-seq atlases (human + mouse) | Embedding-based latent prediction; expression reconstruction | Stronger generalization vs smaller models on gene-function & perturbation tasks |
| **CellFlow** | Klein et al., 2025 | Neural OT + Flow Matching generative framework | Molecular fingerprints (drugs), ESM2 embeddings (proteins) | Cytokine stimulation, gene KO, drugs | Yes | Human PBMCs, zebrafish embryos, organoids | Conditional flow model for perturbed single-cell phenotypes | SOTA prediction across diverse contexts and developmental stages |
| **STATE** | Adduri et al., 2025 | Transformer foundation model for perturbation prediction | Tokenized representations of cells and perturbations | Genetic, signaling, chemical | Yes | >100 M perturbed cells across experiments | Predicts gene expression changes & DE genes | +30 % accuracy gain vs prior methods; generalizes to unseen contexts |
| **CRISP** | Wang et al., 2025 | Contrastive representation learning + Transformer | Learned context-aware perturbation embeddings | Genetic, drug | Limited | >80 M cells (multi-species perturbation compendium) | Learns latent contrastive representations for differential response prediction | Outperforms supervised baselines on cross-context transfer |
| **C2S-Scale** | Rizvi et al., 2025 | Large Language Model (LLM) trained on Cell2Sentence data (27 B params) | Perturbations and cell states represented as text tokens | Genetic, drug, combinatorial | Yes | >50 M cells + biological text corpora | Generates transcriptomic and textual responses; context-conditioned reasoning | Experimental validation of silmitasertib synergy; SOTA performance |
| **Tahoe-x1 (Tx1)** | Gandhi et al., 2025 | Masked-expression transformer (scGPT-style) with drug tokens | Joint embeddings for genes, cells, compounds | Small-molecule (cancer) | Yes | Tahoe-100 M (>100 M cells, 1,100 compounds) | Predicts gene essentiality & perturbation responses | 3–30× more compute-efficient than previous cell-state models |
| **LPM (Large Perturbation Model)** | Miladinovic et al., 2025 | Deep PRC-conditioned architecture (disentangled perturbation–readout–context) | Symbolic representation of perturbation, readout & context | Drugs, genetic | Yes | Multi-omics pooled perturbation experiments | Predicts outcomes of unseen perturbations; infers gene–gene networks | SOTA performance and biological interpretability at scale |

