| Model | Reference | Core Method | Perturbation Encoding | Perturbation Types | Combination Support | Training Data | Output / Prediction Objective | Benchmarking |
|--------|------------|--------------|------------------------|--------------------|---------------------|----------------|--------------------------------|-----------------------|
| **scGen** | Lotfollahi et al., 2019 | VAE + latent space arithmetic | Mean-difference vectors between perturbed and unperturbed distributions | Cytokine stimulation, infection | No | Human PBMCs, intestinal epithelial cells, phagocytes | Predicts full transcriptome under unseen perturbations | - |
| **CellOT** | Bunne et al., 2023 | Neural Optimal Transport (OT) | Implicit | Drugs, gene knockouts | - | statefate, SciPlex-3 | - | - |
| **CPA** | Lotfollahi et al., 2023 | Autoencoder | Learned latent embeddings for drugs, doses, genes | Drugs, genetic | Yes | PBMC, sciPlex 2,3, Combosciplex + cross-species experiment | Predicts post-perturbation expression states | - |
| **Geneformer** | Theodoris et al., 2023, Chen et al., 2024 | Transformer-based single-cell foundation model pretraining | Contextual token embeddings for genes + perturbations | Drug, genetic | Limited | Multi-tissue scRNA-seq atlases (human + mouse) | Embedding-based latent prediction; expression reconstruction | Stronger generalization vs smaller models on gene-function & perturbation tasks |
| **CellFlow** | Klein et al., 2025 | Neural OT + Flow Matching generative framework | Molecular fingerprints (drugs), ESM2 embeddings (proteins) | Cytokine stimulation, gene KO, drugs | Yes | Human PBMCs, zebrafish embryos, organoids | Conditional flow model for perturbed single-cell phenotypes | SOTA prediction across diverse contexts and developmental stages |
| **STATE** | Adduri et al., 2025 | Transformer foundation model for perturbation prediction | Tokenized representations of cells and perturbations | Genetic, signaling, chemical | Yes | >100 M perturbed cells across experiments | Predicts gene expression changes & DE genes | +30 % accuracy gain vs prior methods; generalizes to unseen contexts |
| **CRISP** | Wang et al., 2025 | VAE based contrastive learning | scFMs + VAE, RDKit | Genetic, drug | Limited | NeurIPS (PBMCs), SciPlex3 (cancer cell lines), GBM, PC9 (NSCLC), PBMC-bench | outputs cell type specific perturbed gene expression profile | - |
| **C2S-Scale** | Rizvi et al., 2025 | Decoder-only transformer | Perturbations and cell states represented as text tokens | Genetic, drug, combinatorial | Yes | >50 M cells + biological text corpora | Next token prediction in an autoregressive manner| - |
| **Tx1** | Gandhi et al., 2025 | Masked LM transformer (scGPT-style) | Joint embeddings for genes, cells, compounds | Small-molecule (cancer) | Yes | Tahoe-100 M (>100 M cells, 1,100 compounds) | -| - |
| **LPM** | Miladinovic et al., 2025 | MLP | Symbolic representation of perturbation, readout & context | Drugs, genetic | Yes | single cell + bulk RNASeq | - | compared against six baselines: CPA, GEARS, CatBoost combined with precomputed gene embeddings from STRING, Reactome and Gene2Vec, Geneformer, scGPT and GenePT |

